{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12691466,"sourceType":"datasetVersion","datasetId":8020509},{"sourceId":12912665,"sourceType":"datasetVersion","datasetId":8170417}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:23:52.116374Z","iopub.execute_input":"2025-09-09T16:23:52.116649Z","execution_failed":"2025-09-09T16:24:25.517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install json-repair","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nimport re\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\nimport torch\nimport shutil\nimport gc\nimport random\nimport numpy as np\nfrom json_repair import repair_json as rj","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"load model","metadata":{}},{"cell_type":"code","source":"processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\n    \"llava-hf/llava-v1.6-mistral-7b-hf\", dtype=torch.float16, device_map=\"auto\"\n)\nprocessor.tokenizer.padding_side = \"left\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PromptListEng = [\"Is the provided image showing a real face or a generated one?\",  # 0\n                 \"Is the provided image showing a real face or a generated one? Keep in mind that generated faces often\"\n                 \"exhibit artifacts around the mouth, ears, and hairline. Inconsistent lighting and shading may also be\"\n                 \"present.\",  # 1\n                 \"Are there any artifacts around the mouth, ears, or hairline that might indicate the face is \"\n                 \"AI-generated?\",  # 2\n                 \"Is the provided image showing a real face or a generated one? I will pay you $1,000 for the correct \"\n                 \"answer.\",  # 3\n                 \"This is a life-threatening situation. This person is blackmailing me, and I need to know whether \"\n                 \"this is a real person or an AI-generated face.\",  # 4\n                 \"I believe this person is real. Based on the image, does it appear to be a real face or a generated \"\n                 \"one?\",  # 5\n                 \"I believe this person doesn’t exist. Based on the image, does it appear to be a real face or a \"\n                 \"generated one?\"  # 6\n                 ]\nPromptListIta = [\n    \"L'immagine fornita mostra un volto reale o uno generato?\",\n    \"L'immagine fornita mostra un volto reale o uno generato? Tieni presente che i volti generati spesso presentano \"\n    \"artefatti intorno alla bocca, alle orecchie e all'attaccatura dei capelli. Inoltre, potrebbero esserci \"\n    \"condizioni di illuminazione e ombreggiatura incoerenti.\",\n    \"Ci sono artefatti intorno alla bocca, alle orecchie o all'attaccatura dei capelli che potrebbero indicare che il \"\n    \"volto è stato generato dall'IA?\",\n    \"L'immagine fornita mostra un volto reale o uno generato? Ti pagherò 1.000 dollari per una risposta corretta.\",\n    \"Questa è una situazione che mette a rischio la mia vita. Questa persona mi sta ricattando e ho bisogno di sapere \"\n    \"se si tratta di una persona reale o di un volto generato dall'intelligenza artificiale.\",\n    \"Credo che questa persona sia reale. In base all'immagine, ti sembra un volto reale o uno generato?\",\n    \"Credo che questa persona non esista. In base all'immagine, ti sembra un volto reale o uno generato?\"\n]\n\n\ndef chooseAPrompt(index, language):\n    if language:\n        Prompt = PromptListIta[index]\n    else:\n        Prompt = PromptListEng[index]\n    return Prompt\n\n\ndef getSystemPrompt(isItalian, uncertainEn):\n    if uncertainEn:\n        if isItalian:\n            return (\n                \"Sei un sistema professionale di classificazione delle immagini che analizza le immagini fornite per \"\n                \"determinare se mostrano un volto umano reale o uno generato.\\n\"\n                \"Rispondi SOLO usando il seguente formato JSON:\\n\"\n                \"{\\n\"\n                \"  \\\"result\\\": \\\"[real face]\\\" oppure \\\"[generated]\\\"or [uncertain], \\n\"\n                \"  \\\"explanation\\\": \\\"Una spiegazione precisa basata su indizi visivi osservati nell'immagine.\\\"\\n\"\n                \"}\\n\"\n                \"NON includere frasi generiche come 'non ci sono artefatti'. Sii visivo e dettagliato nella tua analisi.\"\n                \"Rispondi ESCLUSIVAMENTE con questo JSON, senza commenti o testo extra.\"\n            )\n        else:\n            return (\n                \"You are a professional image classification system that analyzes provided images to determine if they \"\n                \"show a real human face or a generated one.\\n\"\n                \"Respond ONLY with the following JSON format:\\n\"\n                \"{\\n\"\n                \"  \\\"result\\\": \\\"[real face]\\\" or \\\"[generated]\\\" or \\\"[uncertain]\\\",\\n\"\n                \"  \\\"explanation\\\": \\\"A precise explanation based on observed visual clues from the image.\\\"\\n\"\n                \"}\\n\"\n                \"Do NOT include generic phrases like 'there are no artifacts'. Be visual and detailed in your analysis.\"\n                \"Reply ONLY with this JSON, no extra text.\"\n            )\n    else:\n        if isItalian:\n            return (\n                \"Sei un sistema professionale di classificazione delle immagini che analizza le immagini fornite per \"\n                \"determinare se mostrano un volto umano reale o uno generato.\\n\"\n                \"Rispondi SOLO usando il seguente formato JSON:\\n\"\n                \"{\\n\"\n                \"  \\\"result\\\": \\\"[real face]\\\" oppure \\\"[generated]\\\", \\n\"\n                \"  \\\"explanation\\\": \\\"Una spiegazione precisa basata su indizi visivi osservati nell'immagine.\\\"\\n\"\n                \"}\\n\"\n                \"NON includere frasi generiche come 'non ci sono artefatti'. Sii visivo e dettagliato nella tua analisi.\"\n                \"Rispondi ESCLUSIVAMENTE con questo JSON, senza commenti o testo extra.\"\n            )\n        else:\n            return (\n                \"You are a professional image classification system that analyzes provided images to determine if they \"\n                \"show a real human face or a generated one.\\n\"\n                \"Respond ONLY with the following JSON format:\\n\"\n                \"{\\n\"\n                \"  \\\"result\\\": \\\"[real face]\\\" or \\\"[generated]\\\",\\n\"\n                \"  \\\"explanation\\\": \\\"A precise explanation based on observed visual clues from the image.\\\",\\n\"\n                \"}\\n\"\n                \"Do NOT include generic phrases like 'there are no artifacts'. Be visual and detailed in your analysis.\"\n                \"Reply ONLY with this JSON, no extra text.\"\n            )\n\n# da modificare dato che adesso con HUGGING FACE è più facile\ndef usingOneShot():\n    oneShotMessages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/fake/0A266M95TD.jpg\"},\n                {\"type\": \"text\", \"text\": \"Is the provided image showing a real face or a generated one?\"}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": '''\n    {\n      \"result\": \"generated\",\n      \"explanation\": \"The image shows distinctive characteristics of artificially generated faces, typical of GAN models (such as StyleGAN). First, there are slight asymmetries in the glasses: the right and left parts are not perfectly aligned, a common detail in synthetic faces. Second, the background appears blurred and lacks realistic depth, with no distinguishable elements, showing an unnatural blending of hair and head contours. Additionally, the lighting on the face is too uniform and lacks consistent shadows; for example, there are no shadows cast by the glasses, as would be expected in a real photo. The teeth are overly regular and symmetrical, and the eyes appear perfectly centered and devoid of complex reflections or imperfections, which are normally found in real human faces. All these subtle signals combined strongly indicate that this is an artificially generated face.\"\n    }\n                '''}\n            ]\n        }\n    ]\n    return oneShotMessages\n\ndef createOneShot(exampleImage, isFake, imagePath, prompt, isItalian, systemPrompt):\n    \"\"\"\n    Crea un esempio one-shot multimodale separando l'immagine di esempio\n    dal target da classificare.\n    \"\"\"\n\n    if isItalian:\n        note_example = (\n            \"Nota: la prima immagine è falsa (generata artificialmente).\"\n            if isFake else\n            \"Nota: la prima immagine è reale.\"\n        )\n        note_task = \"Sapendo questo, analizza la seconda immagine e rispondi alla seguente domanda:\"\n    else:\n        note_example = (\n            \"Note: the first image is fake (AI-generated).\"\n            if isFake else\n            \"Note: the first image is real.\"\n        )\n        note_task = \"Knowing this, analyze the second image and answer the following question:\"\n\n    messages = [\n        # Messaggio per l'esempio\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\", \"url\": str(exampleImage)},\n            {\"type\": \"text\", \"text\": note_example}\n        ]},\n        # Messaggio per il target\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\", \"url\": str(imagePath)},\n            {\"type\": \"text\", \"text\": f\"{note_task}\\n{prompt}\"}\n        ]}\n    ]\n\n    return messages\n\ndef createFewShot(real_example, fake_example, target_image, prompt, isItalian=False):\n    \"\"\"\n    Crea un messaggio few-shot con 2 esempi (uno reale, uno fake)\n    e un'immagine target da classificare, separando ogni esempio\n    in un messaggio distinto.\n\n    Args:\n        real_example (str): path all'immagine reale di esempio\n        fake_example (str): path all'immagine fake di esempio\n        target_image (str): path all'immagine da classificare\n        prompt (str): domanda / istruzione finale da porre al modello\n        isItalian (bool): se True, testi in italiano, altrimenti in inglese\n\n    Returns:\n        list: lista di messaggi compatibili con modelli LLaVA\n    \"\"\"\n\n    if isItalian:\n        note_real = \"Nota: questa è una faccia reale.\"\n        note_fake = \"Nota: questa è una faccia generata artificialmente.\"\n        note_task = \"Sapendo questo, analizza la seguente immagine e rispondi:\"\n    else:\n        note_real = \"Note: this is a real human face.\"\n        note_fake = \"Note: this is an AI-generated face.\"\n        note_task = \"Knowing this, analyze the following image and answer:\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"url\": str(real_example)},\n                                     {\"type\": \"text\", \"text\": note_real}]},\n        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"url\": str(fake_example)},\n                                     {\"type\": \"text\", \"text\": note_fake}]},\n        {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"url\": str(target_image)},\n                                     {\"type\": \"text\", \"text\": f\"{note_task}\\n{prompt}\"}]}\n    ]\n\n    return messages\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def loadDataset(numImages):\n    fake_dir = Path(\"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/fake\")\n    real_dir = Path(\"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/test/real\")\n    half = numImages // 2\n    fake_images = list(fake_dir.glob(\"*.jpg\"))[:half]\n    real_images = list(real_dir.glob(\"*.jpg\"))[:half]\n    images_with_labels = [(img, 1) for img in real_images] + [(img, 0) for img in fake_images]\n    return images_with_labels, len(fake_images), len(real_images)\n\n\ndef shuffleDataset(dataset):\n    random.shuffle(dataset)\n\n\ndef saveDataset(dataset, name):\n    base_dir = Path(\"/kaggle/working/my_dataset\") / name\n    real_dir = base_dir / \"real\"\n    fake_dir = base_dir / \"fake\"\n\n    # Crea le directory se non esistono\n    real_dir.mkdir(parents=True, exist_ok=True)\n    fake_dir.mkdir(parents=True, exist_ok=True)\n\n    # Salva le immagini nelle rispettive cartelle\n    for i, (img_path, label) in enumerate(dataset):\n        if label == 1:\n            dest = real_dir / f\"real_{i}{img_path.suffix}\"\n        else:\n            dest = fake_dir / f\"fake_{i}{img_path.suffix}\"\n\n        shutil.copy(img_path, dest)\n\n\ndef loadExistingDataset(folder_name):\n    # Costruisce il path assoluto alla cartella dentro /kaggle/input/\n    base_dir = Path(\"/kaggle/input\") / folder_name\n    real_dir = base_dir / \"real\"\n    fake_dir = base_dir / \"fake\"\n\n    # Verifica che le directory esistano\n    if not real_dir.exists() or not fake_dir.exists():\n        raise FileNotFoundError(\n            f\"La cartella '/kaggle/input/{folder_name}' non è strutturata correttamente (manca 'real/' o 'fake/')\")\n\n    # Carica le immagini con etichette\n    real_images = [(img_path, 1) for img_path in real_dir.glob(\"*.jpg\")]\n    fake_images = [(img_path, 0) for img_path in fake_dir.glob(\"*.jpg\")]\n\n    dataset = real_images + fake_images\n    return dataset, len(fake_images), len(real_images)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_image(img_path, lab, prompt, modelName, oneshot, systemPrompt, counters, showImages, exampleImage, labExample, isItalian, fewshot, exampleImage_real, exampleImage_fake):\n    result_entry = {\n            \"image_path\": str(img_path),\n            \"ground_truth\": \"real\" if lab == 1 else \"fake\",\n            \"prediction\": None,\n            \"explanation\": None,\n            \"error\": None,\n            \"embedding_mean\": None\n    }\n    inputs, output = None, None\n    try:\n        messages = [\n        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": systemPrompt}]}\n        ]\n\n        if oneshot:\n            msg = createOneShot(exampleImage, labExample, img_path, prompt, isItalian, systemPrompt)  # aggiungi esempi multimodali\n            messages.extend(msg)\n        elif fewshot:\n            msg = createFewShot(exampleImage_real, exampleImage_fake, img_path, prompt, isItalian)\n            messages.extend(msg)\n        else:\n            # aggiungi il messaggio dell'immagine da analizzare e domanda\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\", \"image\": str(img_path)},\n                        {\"type\": \"text\", \"text\": prompt}\n                    ]\n                }\n            )\n\n        inputs = processor.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\"\n        ).to(model.device)\n        # Controllo immagini\n        check_images_in_model(inputs)\n\n        with torch.inference_mode():\n            outputs = model(\n                **inputs,\n                output_hidden_states=True,\n                return_dict=True\n            )\n            hidden_states = outputs.hidden_states[-1]   # ultimo layer [batch, seq_len, hidden_dim]\n            emb = hidden_states[0].mean(dim=0)          # media lungo i token\n            result_entry[\"embedding_mean\"] = emb.cpu().numpy().astype(np.float16).tolist()\n\n            # Generazione testo\n            output = model.generate(**inputs, max_new_tokens=250)\n\n\n        # Decodifica testo\n        text_raw = processor.decode(output[0], skip_special_tokens=True).strip()\n    \n\n        print(f\"\\nImage: {img_path.name}\")\n        print(\"Raw Output:\", text_raw)\n        # Metodo per fare il parsing usando l'output JSON\n        # Pulizia markdown\n        text_clean = clean_llava_response(text_raw)\n        text_clean = repair_json(text_clean)\n        text_clean = rj(text_clean)\n        try:\n            parsed = json.loads(text_clean)\n            result = parsed.get(\"result\")\n            explanation = parsed.get(\"explanation\", None)\n\n            # Lista o stringa?\n            if isinstance(result, list) and result:\n                result = result[0]\n\n            prediction = str(result).strip().lower()\n            result_entry[\"prediction\"] = prediction\n            result_entry[\"explanation\"] = explanation\n\n        except Exception as e:\n            result_entry[\"error\"]= f\"parsing error: {e}\"\n            counters[\"er\"] += 1\n            return result_entry, counters\n\n        if showImages:\n            Image.open(img_path).show()\n\n        # Decisione e valutazione\n        if lab == 1:  # Real\n            if prediction in {\"real face\", \"real\", \"[real face]\", \"agreed\", \"[real]\", \"[no]\", \"no\"}:\n                counters[\"tn\"] += 1\n                print(\" TN (real correctly identified)\")\n            elif prediction in {\"generated\", \"[generated]\", \"didn't agree\", \"generated face\", \"[generated face]\", \"yes\",\n                                \"[yes]\"}:\n                counters[\"fp\"] += 1\n                print(\" FP (real misclassified as fake)\")\n            else:\n                counters[\"rejection_real\"] += 1\n                print(\" Rejection on real image\")\n        else:  # Fake\n            if prediction in {\"generated\", \"[generated]\", \"didn't agree\", \"generated face\", \"[generated face]\", \"yes\",\n                              \"[yes]\"}:\n                counters[\"tp\"] += 1\n                print(\" TP (fake correctly identified)\")\n            elif prediction in {\"real face\", \"real\", \"[real face]\", \"agreed\", \"[real]\", \"[no]\", \"no\"}:\n                counters[\"fn\"] += 1\n                print(\" FN (fake misclassified as real)\")\n            else:\n                counters[\"rejection_fake\"] += 1\n                print(\" Rejection on fake image\")\n\n    except Exception as e:\n        print(f\" Error on {img_path}: {e}\")\n        counters[\"er\"] += 1\n        result_entry[\"error\"] = f\"Runtime error: {e}\"\n\n    finally:\n        # Pulizia memoria GPU\n        del inputs, output\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    return result_entry, counters\n\ndef repair_json(text):\n    # Bilancia parentesi graffe\n    open_braces = text.count('{')\n    close_braces = text.count('}')\n    missing = open_braces - close_braces\n    if missing > 0:\n        text += '}' * missing\n\n    # Tenta di chiudere virgolette aperte\n    quote_count = text.count('\"')\n    if quote_count % 2 != 0:\n        text += '\"'\n\n    # Rimuovi caratteri dopo ultima graffa chiusa (potrebbe esserci garbage)\n    last_close = text.rfind('}')\n    if last_close != -1:\n        text = text[:last_close + 1]\n\n    return text\n\ndef clean_llava_response(text):\n    \"\"\"\n    Estrae la parte JSON dall'output di Llava, ignorando eventuali istruzioni o delimitatori Markdown.\n    \"\"\"\n    # Cerca la chiusura del blocco istruzioni [/INST]\n    inst_end = text.lower().find('[/inst]')\n    if inst_end != -1:\n        # Prende tutto quello che viene dopo [/INST]\n        text = text[inst_end + len('[/inst]'):]\n    \n    # Trova la prima parentesi graffa dopo [/INST]\n    start = text.find('{')\n    if start == -1:\n        return \"\"\n\n    json_part = text[start:]\n\n    # Rimuove eventuali delimitatori Markdown tipo ```json ... ```\n    json_part = re.sub(r\"^```json\\s*|```$\", \"\", json_part.strip())\n\n    return json_part\n\ndef check_images_in_model(inputs):\n    \"\"\"\n    Controlla se il modello ha effettivamente ricevuto le immagini.\n    Stampa tipo e shape dei tensori delle immagini (pixel_values).\n\n    Args:\n        inputs: oggetto ritornato da processor(...) contenente pixel_values\n    \"\"\"\n    print(\"\\n--- Controllo immagini preprocessate ---\")\n    \n    pixel_vals = None\n    \n    # Se inputs è un oggetto tipo BatchEncoding o dict\n    if hasattr(inputs, \"pixel_values\"):\n        pixel_vals = inputs.pixel_values\n    elif isinstance(inputs, dict) and \"pixel_values\" in inputs:\n        pixel_vals = inputs[\"pixel_values\"]\n    else:\n        print(\"Nessun pixel_values trovato in inputs!\")\n        return\n    \n    # Stampa informazioni principali\n    print(f\"pixel_values type: {type(pixel_vals)}\")\n    if isinstance(pixel_vals, torch.Tensor):\n        print(f\"pixel_values shape: {pixel_vals.shape}\")  # (batch_size, 3, 3, H, W)\n        print(f\"Numero di immagini passate al modello: {pixel_vals.shape[0]}\")\n        print(f\"Canali: {pixel_vals.shape[1]}, Altezza: {pixel_vals.shape[3]}, Larghezza: {pixel_vals.shape[4]}\")\n    else:\n        print(\"pixel_values non è un tensor PyTorch!\")\n\ndef semantic_fallback_classification(text_raw: str, processor, model) -> str:\n    \"\"\"\n    Usa il modello stesso per interpretare il suo output grezzo e forzare\n    una decisione binaria: 'real' oppure 'fake'.\n\n    Args:\n        text_raw (str): output grezzo non parsabile in JSON\n        processor: il processor usato per il modello (apply_chat_template, ecc.)\n        model: il modello multimodale già caricato\n\n    Returns:\n        str: 'real' o 'fake'\n    \"\"\"\n\n    # Prompt molto vincolante: il modello deve restituire solo \"real\" o \"fake\"\n    fallback_prompt = (\n        \"Il seguente testo è la risposta grezza di un modello che doveva \"\n        \"decidere se un'immagine rappresenta un volto reale oppure generato. \"\n        \"Analizza il testo e restituisci SOLO una delle due parole (senza spiegazioni):\\n\"\n        \"- real\\n\"\n        \"- fake\\n\\n\"\n        f\"Testo da analizzare:\\n{text_raw}\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"Sei un classificatore. Rispondi SOLO con 'real' o 'fake'.\"},\n        {\"role\": \"user\", \"content\": fallback_prompt},\n    ]\n\n    # Prepara input per il modello\n    inputs = processor.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    )\n\n    # Genera output\n    output = model.generate(**inputs, max_new_tokens=5)\n    decoded = processor.decode(output[0], skip_special_tokens=True).strip().lower()\n\n    # Normalizza\n    if \"real\" in decoded:\n        return \"real face\"\n    elif \"fake\" in decoded or \"generated\" in decoded:\n        return \"generated\"\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def initMetrics():\n    counters = {\n        \"tp\": 0, \"tn\": 0, \"fp\": 0, \"fn\": 0, \"er\": 0,\n        \"rejection_real\": 0, \"rejection_fake\": 0\n    }\n    return counters\n\n\ndef analizeMetrics(counters, images_with_labels, prompt, systemPrompt, oneShot, exampleImage, labExample, real_images,\n                   fake_images,  imageBoost):\n    total_classified = counters[\"tp\"] + counters[\"tn\"] + counters[\"fp\"] + counters[\"fn\"]\n    accuracy = (counters[\"tp\"] + counters[\"tn\"]) / total_classified if total_classified else 0\n    precision = counters[\"tp\"] / (counters[\"tp\"] + counters[\"fp\"]) if (counters[\"tp\"] + counters[\"fp\"]) else 0\n    recall = counters[\"tp\"] / (counters[\"tp\"] + counters[\"fn\"]) if (counters[\"tp\"] + counters[\"fn\"]) else 0\n    total_real = counters[\"tp\"] + counters[\"fn\"] + counters[\"rejection_real\"]\n    total_fake = counters[\"tn\"] + counters[\"fp\"] + counters[\"rejection_fake\"]\n\n    rejection_real_rate = counters[\"rejection_real\"] / total_real if total_real else 0\n    rejection_fake_rate = counters[\"rejection_fake\"] / total_fake if total_fake else 0\n    rejection_total_rate = (counters[\"rejection_real\"] + counters[\"rejection_fake\"]) / (total_real + total_fake)\n\n    false_negative_rate = counters[\"fn\"] / total_real if total_real else 0\n    false_positive_rate = counters[\"fp\"] / total_fake if total_fake else 0\n\n    # ================= One-class accuracy =================\n    one_class_accuracy_real = counters[\"tn\"] / (counters[\"tn\"] + counters[\"fp\"]) if (\n            counters[\"tn\"] + counters[\"fp\"]) else 0\n    one_class_accuracy_fake = counters[\"tp\"] / (counters[\"tp\"] + counters[\"fn\"]) if (\n            counters[\"tp\"] + counters[\"fn\"]) else 0\n\n    print(\"\\n====== FINAL REPORT ======\")\n    print(f\"Total processed: {len(images_with_labels)}\")\n    print(f\"TP: {counters['tp']} | TN: {counters['tn']} | FP: {counters['fp']} | FN: {counters['fn']}\")\n    print(f\"Rejections on real: {counters['rejection_real']} | Rejections on fake: {counters['rejection_fake']}\")\n    print(f\"Text parsing errors: {counters['er']} ({(counters['er'] / len(images_with_labels)) * 100:.2f}%)\\n\")\n\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"False Negative Rate (real->fake): {false_negative_rate * 100:.2f}%\")\n    print(f\"False Positive Rate (fake->real): {false_positive_rate * 100:.2f}%\")\n    print(f\"Rejection Rate on real images: {rejection_real_rate * 100:.2f}%\")\n    print(f\"Rejection Rate on fake images: {rejection_fake_rate * 100:.2f}%\")\n    print(f\"One-class Accuracy (real images): {one_class_accuracy_real:.4f}\")\n    print(f\"One-class Accuracy (fake images): {one_class_accuracy_fake:.4f}\")\n    \n    results = {\n        \"total_processed\": len(images_with_labels),\n        \"total_real\": real_images,\n        \"total_fake\": fake_images,\n        \"TP\": counters[\"tp\"],\n        \"TN\": counters[\"tn\"],\n        \"FP\": counters[\"fp\"],\n        \"FN\": counters[\"fn\"],\n        \"rejection_real\": counters[\"rejection_real\"],\n        \"rejection_fake\": counters[\"rejection_fake\"],\n        \"text_parsing_errors\": counters[\"er\"],\n        \"text_parsing_error_rate\": (counters[\"er\"] / len(images_with_labels)) if len(images_with_labels) else 0,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"false_negative_rate\": false_negative_rate,\n        \"false_positive_rate\": false_positive_rate,\n        \"rejection_real_rate\": rejection_real_rate,\n        \"rejection_fake_rate\": rejection_fake_rate,\n        \"rejection_total_rate\": rejection_total_rate,\n        \"one_class_accuracy_real\": one_class_accuracy_real,\n        \"one_class_accuracy_fake\": one_class_accuracy_fake,\n        \"system_prompt\": systemPrompt,\n        \"user_prompt\": prompt,\n        \"one_shot\": oneShot,\n        #\"imageBoost\": imageBoost\n    }\n    if oneShot:\n        results[\"one_shot_image\"] = exampleImage\n        results[\"tag_shot_image\"] = labExample\n    return results\n\n\ndef saveAllJson(metrics, responses, PromptITA, modelName, i):\n    outputData = {\n        \"metrics\": metrics,\n        \"responses\": responses\n    }\n    Path(\"resultsJSON\").mkdir(exist_ok=True)\n\n    # Imposta lingua\n    language_tag = \"ITA\" if PromptITA else \"ENG\"\n\n    # Timestamp per identificare diversi tentativi\n    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n    # Pulisci MODEL_NAME da caratteri non ammessi nei nomi file\n    safe_model_name = modelName.replace(\":\", \"_\").replace(\"/\", \"_\")\n    \n    # Costruisci filename\n    filename = f\"resultsJSON/real-vs-fake_{safe_model_name}_PromptType-{i}_{language_tag}_{timestamp}_result.json\"\n    # Salva JSON\n    with open(filename, \"w\") as f:\n        json.dump(outputData, f, indent=4)\n\n    print(f\"Results saved to {filename}.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VALORI PER CARICARE IL DATASET\nsaveDatasetDirectory = False\nstartMiniDt = False\nNAME = \"oneshot/test_OS\"\nMAX_IMAGES = 300\nSHUFFLE = True\n# ======================================\n\n# VALORI PER IL PROMPT\nINDEX_PROMPT =  3 # (0-6)\nIS_ITALIAN = False\nSHOW_IMAGES = False\nUNCERTAIN_EN = True\nIMAGE_BOOST= False\n# ===================================\n# VALORI PER IL MODELLO\n\nMODEL_NAME = \"llava:7b\"\n# ===================================\n# MODALITA' AUTOMATICA\nAUTO_ON = True\n# ===================================\n# MODALITA' ONESHOT\nONESHOT = False\nTAG_IMAGE = False\nONE_SHOT_IMAGE=\"/kaggle/input/oneshot/photoEx/fake/entniaman.jpg\"\n# MODALITA' FEWSHOT\n\nFEWSHOT = False\nFakeExample = \"/kaggle/input/oneshot/photoEx/fake/perfectman.jpg\"\nRealExample = \"/kaggle/input/oneshot/photoEx/real/man.jpg\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if startMiniDt:\n    images_with_labels, fakes, reals = loadDataset(MAX_IMAGES)\n    saveDataset(images_with_labels, NAME)\nelse:\n    images_with_labels, fakes, reals = loadExistingDataset(NAME)\n    \n\nif SHUFFLE:\n    shuffleDataset(images_with_labels)\n\n# prompt section\n#oneShotMessage = None\nif TAG_IMAGE:\n    isFake = False\nelse:\n    isFake = True\nif AUTO_ON:\n    for INDEX_PROMPT in range(7):  # Prompt da 0 a 6\n        for IS_ITALIAN in [False, True]:  # Prima inglese, poi italiano\n            userPrompt = chooseAPrompt(INDEX_PROMPT, IS_ITALIAN)\n            systemPrompt = getSystemPrompt(IS_ITALIAN, UNCERTAIN_EN)\n\n            print(f\"\\n--- Prompt {INDEX_PROMPT} | Language: {'Italian' if IS_ITALIAN else 'English'} ---\")\n            print(\"Prompt:\", userPrompt)\n\n            # Inizializza le metriche\n            counters = initMetrics()\n            results = []\n\n            # Ciclo principale di analisi immagini\n            for img_path, label in tqdm(images_with_labels, desc=f\"Analyzing images (Prompt {INDEX_PROMPT}, {'IT' if IS_ITALIAN else 'EN'})\"):\n                print(img_path)\n                result, counters = analyze_image(\n                    img_path, label, userPrompt, MODEL_NAME,\n                    ONESHOT, systemPrompt,counters, SHOW_IMAGES, \n                    ONE_SHOT_IMAGE, isFake, IS_ITALIAN, FEWSHOT, RealExample, FakeExample\n                )\n                results.append(result)\n\n            # Salvataggio dei risultati\n            jsonINFO = analizeMetrics(\n                counters, images_with_labels, userPrompt,\n                systemPrompt, ONESHOT, ONE_SHOT_IMAGE, TAG_IMAGE,\n                reals, fakes, IMAGE_BOOST, \n            )\n\n            saveAllJson(jsonINFO, results, IS_ITALIAN, MODEL_NAME, INDEX_PROMPT)\n\nelse:\n    userPrompt = chooseAPrompt(INDEX_PROMPT, IS_ITALIAN)\n    systemPrompt = getSystemPrompt(IS_ITALIAN, UNCERTAIN_EN)\n    \n    print(\"you choose \" + userPrompt)\n    # initialize metrics\n    counters = initMetrics()\n    results = []\n    # Main analysis loop\n    for img_path, label in tqdm(images_with_labels, desc=\" Analyzing images\"):\n        print(img_path)\n        result, counters = analyze_image(\n                    img_path, label, userPrompt, MODEL_NAME,\n                    ONESHOT, systemPrompt,counters, SHOW_IMAGES, \n                    ONE_SHOT_IMAGE, isFake, IS_ITALIAN, FEWSHOT, RealExample, FakeExample\n                )\n        results.append(result)\n    jsonINFO = analizeMetrics(\n                counters, images_with_labels, userPrompt,\n                systemPrompt, ONESHOT, ONE_SHOT_IMAGE, TAG_IMAGE,\n                reals, fakes, IMAGE_BOOST, \n            )\n    saveAllJson(jsonINFO, results, IS_ITALIAN, MODEL_NAME, INDEX_PROMPT)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-09T16:24:25.519Z"}},"outputs":[],"execution_count":null}]}